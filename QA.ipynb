{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>{'source': 'gutenberg', 'id': '3urfvvm165iantk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version                                               data\n",
       "0        1  {'source': 'wikipedia', 'id': '3zotghdk5ibi9ce...\n",
       "1        1  {'source': 'cnn', 'id': '3wj1oxy92agboo5nlq4r7...\n",
       "2        1  {'source': 'gutenberg', 'id': '3bdcf01ogxu7zdn...\n",
       "3        1  {'source': 'cnn', 'id': '3ewijtffvo7wwchw6rtya...\n",
       "4        1  {'source': 'gutenberg', 'id': '3urfvvm165iantk..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coqa = pd.read_json('http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json')\n",
    "coqa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del coqa[\"version\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required columns in our dataframe\n",
    "cols = [\"text\",\"question\",\"answer\"]\n",
    "#list of lists to create our dataframe\n",
    "comp_list = []\n",
    "for index, row in coqa.iterrows():\n",
    "    for i in range(len(row[\"data\"][\"questions\"])):\n",
    "        temp_list = []\n",
    "        temp_list.append(row[\"data\"][\"story\"])\n",
    "        temp_list.append(row[\"data\"][\"questions\"][i][\"input_text\"])\n",
    "        temp_list.append(row[\"data\"][\"answers\"][i][\"input_text\"])\n",
    "        comp_list.append(temp_list)\n",
    "new_df = pd.DataFrame(comp_list, columns=cols) \n",
    "#saving the dataframe to csv file for further loading\n",
    "new_df.to_csv(\"CoQA_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>When was the Vat formally opened?</td>\n",
       "      <td>It was formally established in 1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what is the library for?</td>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>for what subjects?</td>\n",
       "      <td>history, and law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>and?</td>\n",
       "      <td>philosophy, science and theology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Vatican Apostolic Library (), more commonl...</td>\n",
       "      <td>what was started in 2014?</td>\n",
       "      <td>a  project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The Vatican Apostolic Library (), more commonl...   \n",
       "1  The Vatican Apostolic Library (), more commonl...   \n",
       "2  The Vatican Apostolic Library (), more commonl...   \n",
       "3  The Vatican Apostolic Library (), more commonl...   \n",
       "4  The Vatican Apostolic Library (), more commonl...   \n",
       "\n",
       "                            question                               answer  \n",
       "0  When was the Vat formally opened?  It was formally established in 1475  \n",
       "1           what is the library for?                             research  \n",
       "2                 for what subjects?                     history, and law  \n",
       "3                               and?     philosophy, science and theology  \n",
       "4          what was started in 2014?                           a  project  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"CoQA_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of question and answers:  108647\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of question and answers: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = np.random.randint(0,len(data))\n",
    "question = data[\"question\"][random_num]\n",
    "text = data[\"text\"][random_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 365 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, text)\n",
    "print(\"The input has a total of {} tokens.\".format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        101\n",
      "how        2,129\n",
      "long       2,146\n",
      "did        2,106\n",
      "it         2,009\n",
      "take       2,202\n",
      "to         2,000\n",
      "put        2,404\n",
      "out        2,041\n",
      "?          1,029\n",
      "[SEP]        102\n",
      "(          1,006\n",
      "cnn       13,229\n",
      ")          1,007\n",
      "-          1,011\n",
      "-          1,011\n",
      "a          1,037\n",
      "new        2,047\n",
      "jersey     3,933\n",
      "police     2,610\n",
      "officer    2,961\n",
      "plead     25,803\n",
      "not        2,025\n",
      "guilty     5,905\n",
      "on         2,006\n",
      "friday     5,958\n",
      ",          1,010\n",
      "a          1,037\n",
      "day        2,154\n",
      "after      2,044\n",
      "he         2,002\n",
      "was        2,001\n",
      "arrested   4,727\n",
      "and        1,998\n",
      "charged    5,338\n",
      "with       2,007\n",
      "setting    4,292\n",
      "fire       2,543\n",
      "to         2,000\n",
      "the        1,996\n",
      "house      2,160\n",
      "of         1,997\n",
      "an         2,019\n",
      "edison    17,046\n",
      "police     2,610\n",
      "captain    2,952\n",
      "and        1,998\n",
      "his        2,010\n",
      "family     2,155\n",
      ".          1,012\n",
      "michael    2,745\n",
      "a          1,037\n",
      ".          1,012\n",
      "dot       11,089\n",
      "##ro       3,217\n",
      "was        2,001\n",
      "arrested   4,727\n",
      "on         2,006\n",
      "thursday   9,432\n",
      "at         2,012\n",
      "his        2,010\n",
      "home       2,188\n",
      "in         1,999\n",
      "mana      24,951\n",
      "##la       2,721\n",
      "##pan      9,739\n",
      ",          1,010\n",
      "new        2,047\n",
      "jersey     3,933\n",
      ",          1,010\n",
      "after      2,044\n",
      "an         2,019\n",
      "investigation   4,812\n",
      "by         2,011\n",
      "the        1,996\n",
      "middlesex  13,654\n",
      "county     2,221\n",
      "prosecutor  12,478\n",
      "'          1,005\n",
      "s          1,055\n",
      "office     2,436\n",
      "and        1,998\n",
      "the        1,996\n",
      "monroe     9,747\n",
      "township   3,545\n",
      "police     2,610\n",
      "department   2,533\n",
      ".          1,012\n",
      "that       2,008\n",
      "investigation   4,812\n",
      "determined   4,340\n",
      "that       2,008\n",
      "a          1,037\n",
      "fire       2,543\n",
      "at         2,012\n",
      "the        1,996\n",
      "police     2,610\n",
      "captain    2,952\n",
      "'          1,005\n",
      "s          1,055\n",
      "home       2,188\n",
      "early      2,220\n",
      "on         2,006\n",
      "may        2,089\n",
      "20         2,322\n",
      "had        2,018\n",
      "been       2,042\n",
      "intentionally  15,734\n",
      "started    2,318\n",
      "outside    2,648\n",
      "the        1,996\n",
      "house      2,160\n",
      ".          1,012\n",
      "police     2,610\n",
      ",          1,010\n",
      "em         7,861\n",
      "##t        2,102\n",
      "personnel   5,073\n",
      "and        1,998\n",
      "firefighters  21,767\n",
      "were       2,020\n",
      "called     2,170\n",
      "to         2,000\n",
      "the        1,996\n",
      "two        2,048\n",
      "-          1,011\n",
      "story      2,466\n",
      ",          1,010\n",
      "colonial   5,336\n",
      "-          1,011\n",
      "style      2,806\n",
      "home       2,188\n",
      "of         1,997\n",
      "police     2,610\n",
      "capt      14,408\n",
      ".          1,012\n",
      "mark       2,928\n",
      "and        1,998\n",
      "##er       2,121\n",
      "##ko       3,683\n",
      "shortly    3,859\n",
      "before     2,077\n",
      "4          1,018\n",
      "a          1,037\n",
      ".          1,012\n",
      "m          1,049\n",
      ".          1,012\n",
      "on         2,006\n",
      "may        2,089\n",
      "20         2,322\n",
      ".          1,012\n",
      "and        1,998\n",
      "##er       2,121\n",
      "##ko       3,683\n",
      "was        2,001\n",
      "in         1,999\n",
      "the        1,996\n",
      "house      2,160\n",
      "with       2,007\n",
      "his        2,010\n",
      "wife       2,564\n",
      ",          1,010\n",
      "two        2,048\n",
      "children   2,336\n",
      "and        1,998\n",
      "92         6,227\n",
      "-          1,011\n",
      "year       2,095\n",
      "old        2,214\n",
      "mother     2,388\n",
      ".          1,012\n",
      "the        1,996\n",
      "fire       2,543\n",
      "was        2,001\n",
      "extinguished  27,705\n",
      "within     2,306\n",
      "10         2,184\n",
      "to         2,000\n",
      "15         2,321\n",
      "minutes    2,781\n",
      "and        1,998\n",
      "there      2,045\n",
      "were       2,020\n",
      "no         2,053\n",
      "injuries   6,441\n",
      ",          1,010\n",
      "but        2,021\n",
      "the        1,996\n",
      "house      2,160\n",
      "was        2,001\n",
      "damaged    5,591\n",
      ".          1,012\n",
      "\"          1,000\n",
      "the        1,996\n",
      "investigators  14,766\n",
      "described   2,649\n",
      "it         2,009\n",
      "as         2,004\n",
      "considerable   6,196\n",
      ",          1,010\n",
      "one        2,028\n",
      "part       2,112\n",
      "of         1,997\n",
      "the        1,996\n",
      "house      2,160\n",
      "had        2,018\n",
      "damage     4,053\n",
      "on         2,006\n",
      "the        1,996\n",
      "first      2,034\n",
      "and        1,998\n",
      "second     2,117\n",
      "floor      2,723\n",
      "of         1,997\n",
      "the        1,996\n",
      "home       2,188\n",
      ",          1,010\n",
      "\"          1,000\n",
      "said       2,056\n",
      "jim        3,958\n",
      "o          1,051\n",
      "'          1,005\n",
      "neil       6,606\n",
      ",          1,010\n",
      "spokesman  14,056\n",
      "for        2,005\n",
      "the        1,996\n",
      "middlesex  13,654\n",
      "county     2,221\n",
      "prosecutor  12,478\n",
      "'          1,005\n",
      "s          1,055\n",
      "office     2,436\n",
      ".          1,012\n",
      "dot       11,089\n",
      "##ro       3,217\n",
      ",          1,010\n",
      "35         3,486\n",
      ",          1,010\n",
      "was        2,001\n",
      "charged    5,338\n",
      "with       2,007\n",
      "five       2,274\n",
      "counts     9,294\n",
      "of         1,997\n",
      "attempted   4,692\n",
      "murder     4,028\n",
      "and        1,998\n",
      "one        2,028\n",
      "count      4,175\n",
      "of         1,997\n",
      "aggravated  25,817\n",
      "arson     24,912\n",
      ",          1,010\n",
      "authorities   4,614\n",
      "said       2,056\n",
      ".          1,012\n",
      "a          1,037\n",
      "nine       3,157\n",
      "-          1,011\n",
      "year       2,095\n",
      "veteran    8,003\n",
      "of         1,997\n",
      "the        1,996\n",
      "edison    17,046\n",
      "police     2,610\n",
      "department   2,533\n",
      ",          1,010\n",
      "he         2,002\n",
      "was        2,001\n",
      "suspended   6,731\n",
      "from       2,013\n",
      "his        2,010\n",
      "job        3,105\n",
      "with       2,007\n",
      "pay        3,477\n",
      ".          1,012\n",
      "dot       11,089\n",
      "##ro       3,217\n",
      "'          1,005\n",
      "s          1,055\n",
      "lawyer     5,160\n",
      ",          1,010\n",
      "lawrence   5,623\n",
      "bitter     8,618\n",
      "##man      2,386\n",
      ",          1,010\n",
      "said       2,056\n",
      "that       2,008\n",
      "his        2,010\n",
      "client     7,396\n",
      "is         2,003\n",
      "\"          1,000\n",
      "in         1,999\n",
      "shock      5,213\n",
      "\"          1,000\n",
      "and        1,998\n",
      "had        2,018\n",
      "told       2,409\n",
      "bitter     8,618\n",
      "##man      2,386\n",
      "that       2,008\n",
      "\"          1,000\n",
      "he         2,002\n",
      "can        2,064\n",
      "'          1,005\n",
      "t          1,056\n",
      "believe    2,903\n",
      "he         2,002\n",
      "'          1,005\n",
      "s          1,055\n",
      "being      2,108\n",
      "arrested   4,727\n",
      ".          1,012\n",
      "\"          1,000\n",
      "superior   6,020\n",
      "court      2,457\n",
      "judge      3,648\n",
      "bradley    8,981\n",
      "fe        10,768\n",
      "##ren      7,389\n",
      "##cz      27,966\n",
      "on         2,006\n",
      "friday     5,958\n",
      "upheld    16,813\n",
      "previously   3,130\n",
      "set        2,275\n",
      "bail      15,358\n",
      "conditions   3,785\n",
      "on         2,006\n",
      "friday     5,958\n",
      ",          1,010\n",
      "which      2,029\n",
      "included   2,443\n",
      "a          1,037\n",
      "$          1,002\n",
      "5          1,019\n",
      "million    2,454\n",
      "bail      15,358\n",
      ".          1,012\n",
      "if         2,065\n",
      "the        1,996\n",
      "bail      15,358\n",
      "is         2,003\n",
      "posted     6,866\n",
      ",          1,010\n",
      "dot       11,089\n",
      "##ro       3,217\n",
      "must       2,442\n",
      "surrender   7,806\n",
      "his        2,010\n",
      "firearms  13,780\n",
      "and        1,998\n",
      "passport  12,293\n",
      "and        1,998\n",
      "may        2,089\n",
      "not        2,025\n",
      "have       2,031\n",
      "any        2,151\n",
      "contact    3,967\n",
      "with       2,007\n",
      "the        1,996\n",
      "victims    5,694\n",
      ".          1,012\n",
      "[SEP]        102\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    print('{:8}{:8,}'.format(token,id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEP token index:  10\n",
      "Number of tokens in segment A:  11\n",
      "Number of tokens in segment B:  354\n"
     ]
    }
   ],
   "source": [
    "#first occurence of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(\"SEP token index: \", sep_idx)\n",
    "#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0\n",
    "num_seg_a = sep_idx+1\n",
    "print(\"Number of tokens in segment A: \", num_seg_a)\n",
    "#number of tokens in segment B (text)\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(\"Number of tokens in segment B: \", num_seg_b)\n",
    "#creating the segment ids\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "#making sure that every input token has a segment id\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
    "output = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "How long did it take to put out?\n",
      "\n",
      "Answer:\n",
      "10 to 15 minutes.\n"
     ]
    }
   ],
   "source": [
    "#tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "if answer_end >= answer_start:\n",
    "    answer = \" \".join(tokens[answer_start:answer_end+1])\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "    \n",
    "print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "print(\"\\nAnswer:\\n{}.\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokens[answer_start]\n",
    "for i in range(answer_start+1, answer_end+1):\n",
    "    if tokens[i][0:2] == \"##\":\n",
    "        answer += tokens[i][2:]\n",
    "    else:\n",
    "        answer += \" \" + tokens[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "    print('Question:', question)\n",
    "    #tokenize question and text as a pair\n",
    "    input_ids = tokenizer.encode(question, text)\n",
    "    \n",
    "    #string version of tokenized ids\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    #segment IDs\n",
    "    #first occurence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "    #number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1\n",
    "    #number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    #list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    #model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "    \n",
    "    #reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "    answer = '[CLS]'\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    print(\"Predicted answer:\")\n",
    "    print(answer.capitalize())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where was the Auction held?\n",
      "Predicted answer:\n",
      "Hard rock cafe in new york ' s times square\n",
      "\n",
      "Original answer:\n",
      " Hard Rock Cafe\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \\\"Motown 25,\\\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \\\"Clyde\\\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\\\" Orange said. \\\"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\\\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
    "question = \"Where was the Auction held?\"\n",
    "question_answer(question, text)\n",
    "#original answer from the dataset\n",
    "print(\"Original answer:\\n\", data.loc[data[\"question\"] == question][\"answer\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who?\n",
      "Predicted answer:\n",
      "Me\n",
      "\n",
      "Question: New text 📝\n",
      "Predicted answer:\n",
      "Unable to find the answer to your question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text = '''My Wonderful Family\n",
    "# I live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital. My brothers are very smart and work hard in school. My sister is a nervous girl, but she is very kind. My grandmother also lives with us. She came from Italy when I was two years old. She has grown old, but she is still very strong. She cooks the best food!\n",
    "\n",
    "# My family is very important to me. We do lots of things together. My brothers and I like to go on long walks in the mountains. My sister likes to cook with my grandmother. On the weekends we all play board games together. We laugh and always have a good time. I love my family very much.'''\n",
    "\n",
    "# questions = [\n",
    "#     'My mother is a...',\n",
    "#     'My house is near the...',\n",
    "#     'How old was I when my grandmother came?',\n",
    "#     'On the weekends, we...',\n",
    "#     'My sister is kind, but also...'\n",
    "# ]\n",
    "\n",
    "# text = '''Spanish Flu Pandemic of 1918\n",
    "# The deadliest virus in modern history, perhaps of all time, was the 1918 Spanish Flu. It killed about 20 to 50 million people worldwide, perhaps more. The total death toll is unknown because medical records were not kept in many areas.\n",
    "\n",
    "# The pandemic hit during World War I and devastated military troops. In the United States, for instance, more servicemen were killed from the flu than from the war itself. The Spanish flu was fatal to a higher proportion of young adults than most flu viruses.\n",
    "\n",
    "# The pandemic started mildly, in the spring of 1918, but was followed by a much more severe wave in the fall of 1918. The war likely contributed to the devastating mortality numbers, as large outbreaks occurred in military forces living in close quarters. Poor nutrition and the unsanitary conditions of war camps had an effect.\n",
    "\n",
    "# A third wave occurred in the winter and spring of 1919, and a fourth, smaller wave occurred in a few areas in spring 1920. Initial symptoms of the flu were typical: sore throat, headache, and fever. The flu often progressed rapidly to cause severe pneumonia and sometimes hemorrhage in the lungs and mucus membranes. A characteristic feature of severe cases of the Spanish Flu was heliotrope cyanosis, where the patient’s face turned blue from lack of oxygen in the cells. Death usually followed within hours or days.\n",
    "\n",
    "# Modern medicine such as vaccines, antivirals, and antibiotics for secondary infections were not available at that time, so medical personnel couldn’t do much more than try to relieve symptoms.\n",
    "\n",
    "# The flu ended when it had infected enough people that those who were susceptible had either died or developed immunity.'''\n",
    "\n",
    "# questions = [\n",
    "#     'Which pandemic is the deadliest in modern history?',\n",
    "#     'The Spanish Flu pandemic occurred during which war?',\n",
    "#     'Where did the Spanish flu originate?',\n",
    "#     'Why are total deaths for the Spanish Flu not known?',\n",
    "#     'When was the first wave of the Spanish Flu pandemic?',\n",
    "#     'What contributed to deaths from the flu in military personnel?',\n",
    "#     'What is a characteristic feature of serious cases of the Spanish Flu?',\n",
    "#     'What caused the Spanish Flu pandemic to end?'\n",
    "# ]\n",
    "\n",
    "# text = '''G-8 and His Battle Aces was an American air-war pulp magazine published from 1930 to 1944. It was one of the first four magazines launched by Popular Publications when it began operations in 1930, and first appeared for just over two years under the title Battle Aces. The success of Street & Smith's The Shadow, a hero pulp (a magazine with a lead novel in each issue featuring a single character), led Popular to follow suit in 1933 by relaunching Battle Aces as a hero pulp: the new title was G-8 and His Battle Aces, and the hero, G-8, was a top pilot and a spy. Robert J. Hogan wrote the lead novels for all the G-8 stories, which were set in World War I. Hogan's plots featured the Germans threatening the Allied forces with extraordinary or fantastic schemes, such as giant bats, zombies, and Martians. He often contributed stories to the magazines as well as the lead novel, though not all the short stories were by him. The cover illustrations, by Frederick Blakeslee, were noted for their fidelity to actual planes flown in World War I.'''\n",
    "\n",
    "# questions = [\n",
    "#     'Who wrote for the G-8 stories?'\n",
    "# ]\n",
    "\n",
    "text = 'hello, it’s me'\n",
    "\n",
    "questions = [\n",
    "    'who?',\n",
    "    'New text 📝'\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    question_answer(question, text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e06593dda434c08a0c60ba781c86e90d517951142dd325300ca8016016a4b7d1"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
